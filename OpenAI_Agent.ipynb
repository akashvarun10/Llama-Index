{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building OpenAI Agent using Llama-Index\n",
    "\n",
    "Testing and exploring agents in llamaindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Sequence, List\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.tools import BaseTool, FunctionTool\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for testing purpose let's define some very simple calculator tools for our agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiple two integers and returns the result integer\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers and returns the result integer\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "add_tool = FunctionTool.from_defaults(fn=add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Definition\n",
    "\n",
    "Now, we define our agent thatâ€™s capable of holding a conversation and calling tools in under 50 lines of code.\n",
    "\n",
    "The meat of the agent logic is in the chat method. At a high-level, there are 3 steps:\n",
    "\n",
    "Call OpenAI to decide which tool (if any) to call and with what arguments.\n",
    "\n",
    "Call the tool with the arguments to obtain an output\n",
    "\n",
    "Call OpenAI to synthesize a response from the conversation context and the tool output.\n",
    "\n",
    "The reset method simply resets the conversation context, so we can start another conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourOpenAIAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tools: Sequence[BaseTool] = [],\n",
    "        llm: OpenAI = OpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\"),\n",
    "        chat_history: List[ChatMessage] = [],\n",
    "    ) -> None:\n",
    "        self._llm = llm\n",
    "        self._tools = {tool.metadata.name: tool for tool in tools}\n",
    "        self._chat_history = chat_history\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self._chat_history = []\n",
    "\n",
    "    def chat(self, message: str) -> str:\n",
    "        chat_history = self._chat_history\n",
    "        chat_history.append(ChatMessage(role=\"user\", content=message))\n",
    "        tools = [\n",
    "            tool.metadata.to_openai_tool() for _, tool in self._tools.items()\n",
    "        ]\n",
    "\n",
    "        ai_message = self._llm.chat(chat_history, tools=tools).message\n",
    "        additional_kwargs = ai_message.additional_kwargs\n",
    "        chat_history.append(ai_message)\n",
    "\n",
    "        tool_calls = ai_message.additional_kwargs.get(\"tool_calls\", None)\n",
    "        # parallel function calling is now supported\n",
    "        if tool_calls is not None:\n",
    "            for tool_call in tool_calls:\n",
    "                function_message = self._call_function(tool_call)\n",
    "                chat_history.append(function_message)\n",
    "                ai_message = self._llm.chat(chat_history).message\n",
    "                chat_history.append(ai_message)\n",
    "\n",
    "        return ai_message.content\n",
    "\n",
    "    def _call_function(self, tool_call: dict) -> ChatMessage:\n",
    "        id_ = tool_call[\"id\"]\n",
    "        function_call = tool_call[\"function\"]\n",
    "        tool = self._tools[function_call[\"name\"]]\n",
    "        output = tool(**json.loads(function_call[\"arguments\"]))\n",
    "        return ChatMessage(\n",
    "            name=function_call[\"name\"],\n",
    "            content=str(output),\n",
    "            role=\"tool\",\n",
    "            additional_kwargs={\n",
    "                \"tool_call_id\": id_,\n",
    "                \"name\": function_call[\"name\"],\n",
    "            },\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = YourOpenAIAgent(tools=[multiply_tool, add_tool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.chat(\"Hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Better way to implement an OpenAI Agent :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from llama_index.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo-0613\")\n",
    "agent = OpenAIAgent.from_tools(\n",
    "    [multiply_tool, add_tool], llm=llm, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What is (121 * 3) + 42?\n",
      "=== Calling Function ===\n",
      "Calling function: multiply with args: {\n",
      "  \"a\": 121,\n",
      "  \"b\": 3\n",
      "}\n",
      "Got output: 363\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: add with args: {\n",
      "  \"a\": 363,\n",
      "  \"b\": 42\n",
      "}\n",
      "Got output: 405\n",
      "========================\n",
      "\n",
      "(121 * 3) + 42 is equal to 405.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"What is (121 * 3) + 42?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ToolOutput(content='363', tool_name='multiply', raw_input={'args': (), 'kwargs': {'a': 121, 'b': 3}}, raw_output=363), ToolOutput(content='405', tool_name='add', raw_input={'args': (), 'kwargs': {'a': 363, 'b': 42}}, raw_output=405)]\n"
     ]
    }
   ],
   "source": [
    "# inspect sources\n",
    "print(response.sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Async Chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What is 121 * 3?\n",
      "=== Calling Function ===\n",
      "Calling function: multiply with args: {\n",
      "  \"a\": 121,\n",
      "  \"b\": 3\n",
      "}\n",
      "Got output: 363\n",
      "========================\n",
      "\n",
      "121 * 3 is equal to 363.\n"
     ]
    }
   ],
   "source": [
    "response = await agent.achat(\"What is 121 * 3?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What is 121 * 2? Once you have the answer, use that number to write a story about a group of mice.\n",
      "=== Calling Function ===\n",
      "Calling function: multiply with args: {\n",
      "  \"a\": 121,\n",
      "  \"b\": 2\n",
      "}\n",
      "Got output: 242\n",
      "========================\n",
      "\n",
      "121 * 2 is equal to 242.\n",
      "\n",
      "Once upon a time, in a small village, there was a group of mice who lived happily in a cozy little burrow. The leader of the group was a wise and courageous mouse named Milo. Milo was known for his intelligence and his ability to solve problems.\n",
      "\n",
      "One day, Milo gathered all the mice together and announced that they were facing a shortage of food. The mice were worried and didn't know what to do. But Milo had a plan. He had heard about a bountiful cornfield located just a few miles away from their burrow.\n",
      "\n",
      "With determination in his eyes, Milo rallied the mice and led them on a journey to the cornfield. They traveled through fields, crossed rivers, and climbed hills, all while staying together as a united group.\n",
      "\n",
      "When they finally reached the cornfield, they were amazed by the sight of endless rows of tall, golden cornstalks. The mice wasted no time and started gathering as much corn as they could. They worked tirelessly, filling their tiny paws with the delicious grains.\n",
      "\n",
      "Thanks to their teamwork and Milo's guidance, the mice were able to collect an impressive amount of corn. They filled their burrow with the golden treasure, ensuring that they would have enough food to survive the winter.\n",
      "\n",
      "As the cold months arrived, the mice huddled together in their warm burrow, feasting on the corn they had gathered. They were grateful for Milo's leadership and the strength of their unity.\n",
      "\n",
      "The story of Milo and the group of mice spread throughout the village, inspiring other animals to work together and overcome challenges. Milo became a legend, and the mice lived happily ever after, knowing that they could achieve anything as long as they stood together.\n",
      "\n",
      "And so, the tale of the group of mice and their journey to the cornfield became a timeless story of courage, determination, and the power of unity."
     ]
    }
   ],
   "source": [
    "response = agent.stream_chat(\n",
    "    \"What is 121 * 2? Once you have the answer, use that number to write a\"\n",
    "    \" story about a group of mice.\"\n",
    ")\n",
    "\n",
    "response_gen = response.response_gen\n",
    "\n",
    "for token in response_gen:\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Async Streaming Chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What is 121 + 8? Once you have the answer, use that number to write a story about a group of mice.\n",
      "=== Calling Function ===\n",
      "Calling function: add with args: {\n",
      "  \"a\": 121,\n",
      "  \"b\": 8\n",
      "}\n",
      "Got output: 129\n",
      "========================\n",
      "\n",
      "121 + 8 is equal to 129.\n",
      "\n",
      "Once upon a time, in a cozy little mouse hole, there lived a group of 129 mice. These mice were known for their intelligence and resourcefulness. They had built a thriving community within the walls of an old farmhouse.\n",
      "\n",
      "Every day, the mice would venture out of their hole in search of food and adventure. They would scurry through the kitchen, carefully avoiding the watchful eyes of the cat. They would nibble on crumbs left behind by the humans and gather seeds from the pantry.\n",
      "\n",
      "One day, as the mice were exploring a new part of the farmhouse, they stumbled upon a hidden treasure trove of cheese. It was a room filled with shelves stacked high with all kinds of cheese - cheddar, Swiss, Gouda, and even some exotic varieties.\n",
      "\n",
      "The mice couldn't believe their luck. They quickly devised a plan to transport the cheese back to their mouse hole. Working together, they formed a chain, passing the cheese from one mouse to another until it reached their cozy little home.\n",
      "\n",
      "With their newfound abundance of cheese, the mice celebrated and feasted for days. They threw a grand cheese party, inviting all the mice from neighboring mouse holes. The aroma of cheese filled the air as the mice danced and sang in joy.\n",
      "\n",
      "Word of the mice's cheese party spread throughout the farmhouse, and even the humans couldn't resist joining in the fun. They watched in amazement as the tiny creatures orchestrated a magnificent celebration.\n",
      "\n",
      "From that day forward, the mice of the farmhouse were known far and wide for their bravery, unity, and love for cheese. They became a legend, inspiring other mice communities to come together and create their own stories of triumph.\n",
      "\n",
      "And so, the group of 129 mice lived happily ever after, continuing to explore, discover, and share their love for cheese with the world."
     ]
    }
   ],
   "source": [
    "response = await agent.astream_chat(\n",
    "    \"What is 121 + 8? Once you have the answer, use that number to write a\"\n",
    "    \" story about a group of mice.\"\n",
    ")\n",
    "\n",
    "response_gen = response.response_gen\n",
    "\n",
    "async for token in response.async_response_gen():\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent with Personality\n",
    "You can specify a system prompt to give the agent additional instruction or personality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.prompts.system import SHAKESPEARE_WRITING_ASSISTANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo-0613\")\n",
    "\n",
    "agent = OpenAIAgent.from_tools(\n",
    "    [multiply_tool, add_tool],\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    system_prompt=SHAKESPEARE_WRITING_ASSISTANT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Hi\n",
      "Greetings, fair traveler! How may I assist thee on this fine day?\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"Hi\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me a story\n",
      "Of course, dear friend! Allow me to weave a tale for thee. Once upon a time, in a land far away, there lived a valiant knight named Sir William. He was known throughout the kingdom for his bravery and noble heart.\n",
      "\n",
      "One fateful day, a messenger arrived at the castle with news of a fearsome dragon terrorizing the nearby village. The villagers were in great distress, for the dragon had been causing havoc and destruction wherever it went.\n",
      "\n",
      "Sir William, hearing of their plight, knew he must embark on a quest to vanquish the dragon and bring peace back to the land. He donned his shining armor, mounted his trusty steed, and set off on his noble mission.\n",
      "\n",
      "Through treacherous forests and across perilous rivers, Sir William journeyed, guided by his unwavering determination. Along the way, he encountered many challenges and faced dangerous creatures, but his courage never wavered.\n",
      "\n",
      "Finally, after days of travel, Sir William reached the village that had been ravaged by the dragon's fiery breath. The villagers, filled with hope at his arrival, gathered around him, seeking solace and protection.\n",
      "\n",
      "With his sword held high and his heart filled with valor, Sir William confronted the mighty dragon. A fierce battle ensued, with flames and steel clashing in the air. The dragon, sensing the knight's unwavering resolve, fought with all its might.\n",
      "\n",
      "But Sir William, with his skill and bravery, proved to be a formidable opponent. With a swift stroke of his sword, he struck the dragon's vulnerable spot, causing it to roar in agony. The villagers watched in awe as the dragon fell to the ground, defeated.\n",
      "\n",
      "The village rejoiced, for their hero had saved them from the clutches of the fearsome dragon. Sir William was hailed as a savior, his name forever etched in the annals of their history.\n",
      "\n",
      "And so, dear friend, the tale of Sir William, the valiant knight who vanquished the dragon, shall be told for generations to come. May it inspire thee to face thy own challenges with courage and determination.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"Tell me a story\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
